"""
Textbook Quality Filter — Inspired by Microsoft's phi-1 "Textbooks Are All You Need"

Applies heuristic quality filtering to identify "textbook-quality" training
samples: self-contained, instructive, and algorithmically rich content.

Compliance:
    - No cross-domain scalarization (Invariant Optimization Tripwire 3)
    - Each quality dimension gated independently via per-dimension thresholds
    - Uses admissibility (pass/fail per dimension), not scalar reward
"""

import re
import math
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field


@dataclass
class QualityReport:
    """Quality assessment for a single training sample.
    
    Each dimension is assessed independently — no cross-domain scalarization.
    Admissibility requires ALL dimensions to pass their respective thresholds.
    """
    is_admissible: bool = False
    self_contained: float = 0.0
    instructive: float = 0.0
    algorithmic: float = 0.0
    clarity: float = 0.0
    dimension_gates: Dict[str, bool] = field(default_factory=dict)
    flags: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'is_admissible': self.is_admissible,
            'self_contained': round(self.self_contained, 3),
            'instructive': round(self.instructive, 3),
            'algorithmic': round(self.algorithmic, 3),
            'clarity': round(self.clarity, 3),
            'dimension_gates': self.dimension_gates,
            'flags': self.flags,
        }


# Keywords that indicate algorithmic / instructive content
ALGORITHMIC_KEYWORDS = {
    'algorithm', 'sort', 'search', 'tree', 'graph', 'hash', 'queue', 'stack',
    'recursive', 'dynamic programming', 'binary search', 'linked list',
    'complexity', 'O(n)', 'O(log', 'breadth-first', 'depth-first',
    'memoize', 'backtrack', 'greedy', 'divide and conquer',
    'matrix', 'vector', 'tensor', 'gradient', 'optimization',
    'probability', 'statistics', 'regression', 'classification',
    'iterate', 'convergence', 'approximation', 'numerical',
}

# Keywords that indicate boilerplate / low-quality
BOILERPLATE_KEYWORDS = {
    'todo', 'fixme', 'hack', 'workaround', 'placeholder',
    'autogenerated', 'auto-generated', 'do not edit',
    'pylint: disable', 'noqa', 'pragma: no cover',
}

# Common import patterns that suggest external dependencies
HEAVY_DEPENDENCY_PATTERNS = [
    r'from\s+\w+\.vendor\.',
    r'import\s+setuptools',
    r'from\s+_?compat\s+import',
    r'# -\*- coding:',
]


class TextbookFilter:
    """
    Heuristic quality classifier that identifies "textbook-quality" content.
    
    Quality is assessed across four INDEPENDENT dimensions, each with its own
    admissibility threshold. No cross-domain scalarization — each dimension
    must independently pass its gate for the sample to be admissible.
    
    Textbook quality means ALL of:
    1. Self-contained: Can be understood without external context
    2. Instructive: Teaches a concept or demonstrates a technique
    3. Algorithmic: Involves meaningful computation or reasoning
    4. Clear: Well-structured with explanatory comments or documentation
    """
    
    def __init__(
        self,
        self_contained_threshold: float = 0.3,
        instructive_threshold: float = 0.3,
        algorithmic_threshold: float = 0.15,
        clarity_threshold: float = 0.3,
    ):
        self.thresholds = {
            'self_contained': self_contained_threshold,
            'instructive': instructive_threshold,
            'algorithmic': algorithmic_threshold,
            'clarity': clarity_threshold,
        }
    
    def assess(self, text: str, source: str = '') -> QualityReport:
        """
        Assess the quality of a training sample.
        
        Each dimension is scored independently and gated against its
        own threshold. Admissibility requires ALL dimensions to pass.
        No cross-domain scalarization.
        
        Args:
            text: The text content to assess
            source: Source identifier (e.g., 'github_repos', 'dolfin')
            
        Returns:
            QualityReport with per-dimension gates and admissibility
        """
        report = QualityReport()
        
        if not text or len(text.strip()) < 50:
            report.flags.append('too_short')
            return report
        
        # Detect content type
        is_code = self._is_code(text)
        
        if is_code:
            report = self._assess_code(text, report)
        else:
            report = self._assess_instruction(text, report)
        
        # Per-dimension gating — each dimension must independently pass
        report.dimension_gates = {
            dim: getattr(report, dim) >= threshold
            for dim, threshold in self.thresholds.items()
        }
        
        # Admissible only if ALL dimension gates pass
        report.is_admissible = all(report.dimension_gates.values())
        
        return report
    
    def filter_batch(
        self,
        texts: List[str],
        source: str = ''
    ) -> List[Dict[str, Any]]:
        """
        Filter a batch of texts, returning those that pass all dimension gates.
        
        Returns list of dicts with 'text', 'admissible', and 'report' keys.
        """
        results = []
        
        for text in texts:
            report = self.assess(text, source)
            if report.is_admissible:
                results.append({
                    'text': text,
                    'admissible': True,
                    'report': report.to_dict(),
                })
        
        return results
    
    def _is_code(self, text: str) -> bool:
        """Detect if text is source code."""
        code_indicators = 0
        if 'def ' in text:
            code_indicators += 1
        if 'class ' in text:
            code_indicators += 1
        if 'import ' in text:
            code_indicators += 1
        if 'return ' in text:
            code_indicators += 1
        if re.search(r'^\s*(if|for|while|try)\s', text, re.MULTILINE):
            code_indicators += 1
        if text.count('    ') > 3 or text.count('\t') > 3:
            code_indicators += 1
        
        return code_indicators >= 3
    
    def _assess_code(self, text: str, report: QualityReport) -> QualityReport:
        """Assess code quality using textbook criteria."""
        lines = text.split('\n')
        
        # --- Self-containment ---
        # Fewer imports = more self-contained
        import_count = sum(1 for l in lines if l.strip().startswith(('import ', 'from ')))
        total_lines = len(lines)
        import_ratio = import_count / max(total_lines, 1)
        
        # Check for heavy external dependencies
        heavy_deps = sum(1 for p in HEAVY_DEPENDENCY_PATTERNS if re.search(p, text))
        
        report.self_contained = max(0, 1.0 - import_ratio * 3 - heavy_deps * 0.3)
        
        # Penalize test/config boilerplate
        if any(kw in text.lower() for kw in BOILERPLATE_KEYWORDS):
            report.self_contained *= 0.5
            report.flags.append('boilerplate_detected')
        
        # --- Instructiveness ---
        has_docstrings = '"""' in text or "'''" in text
        has_comments = sum(1 for l in lines if l.strip().startswith('#')) > 2
        has_type_hints = ': ' in text and '->' in text
        
        report.instructive = 0.0
        if has_docstrings:
            report.instructive += 0.4
        if has_comments:
            report.instructive += 0.3
        if has_type_hints:
            report.instructive += 0.2
        if 'example' in text.lower() or 'usage' in text.lower():
            report.instructive += 0.1
        
        # --- Algorithmic richness ---
        algo_hits = sum(1 for kw in ALGORITHMIC_KEYWORDS if kw in text.lower())
        report.algorithmic = min(algo_hits * 0.15, 1.0)
        
        # Function/class density
        func_count = text.count('def ')
        class_count = text.count('class ')
        if 2 <= func_count <= 15:
            report.algorithmic = min(report.algorithmic + 0.2, 1.0)
        if class_count >= 1:
            report.algorithmic = min(report.algorithmic + 0.1, 1.0)
        
        # --- Clarity ---
        # Reasonable line length
        long_lines = sum(1 for l in lines if len(l) > 120)
        long_ratio = long_lines / max(total_lines, 1)
        
        # Reasonable file size (sweet spot: 50-500 lines)
        size_score = 1.0
        if total_lines < 20:
            size_score = 0.3
        elif total_lines < 50:
            size_score = 0.7
        elif total_lines > 500:
            size_score = max(0.3, 1.0 - (total_lines - 500) / 1000)
        
        report.clarity = max(0, size_score - long_ratio * 0.5)
        
        return report
    
    def _assess_instruction(self, text: str, report: QualityReport) -> QualityReport:
        """Assess instruction/conversational content quality."""
        
        # --- Self-containment ---
        # Does it include both question and answer?
        has_qa = any(marker in text.lower() for marker in 
                    ['question:', 'answer:', 'instruction:', 'output:', 'user:', 'assistant:'])
        
        report.self_contained = 0.8 if has_qa else 0.4
        
        # --- Instructiveness ---
        word_count = len(text.split())
        
        # Penalize very short or very long responses
        if word_count < 20:
            report.instructive = 0.2
            report.flags.append('too_brief')
        elif word_count > 2000:
            report.instructive = 0.5
            report.flags.append('too_verbose')
        else:
            # Medium-length, likely instructive
            report.instructive = min(0.3 + word_count / 500, 0.9)
        
        # Bonus for structured content
        if any(marker in text for marker in ['1.', '2.', '- ', '* ', '```']):
            report.instructive = min(report.instructive + 0.15, 1.0)
        
        # --- Algorithmic ---
        algo_hits = sum(1 for kw in ALGORITHMIC_KEYWORDS if kw in text.lower())
        report.algorithmic = min(algo_hits * 0.12, 1.0)
        
        # Code blocks in instruction text are high-quality signal
        code_blocks = text.count('```')
        if code_blocks >= 2:
            report.algorithmic = min(report.algorithmic + 0.3, 1.0)
        
        # --- Clarity ---
        # Check for coherent sentence structure
        sentences = text.split('.')
        avg_sentence_len = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)
        
        if 5 < avg_sentence_len < 30:
            report.clarity = 0.8
        elif avg_sentence_len <= 5:
            report.clarity = 0.4
        else:
            report.clarity = 0.5
        
        # Penalize repetition
        words = text.lower().split()
        if words:
            unique_ratio = len(set(words)) / len(words)
            if unique_ratio < 0.3:
                report.clarity *= 0.5
                report.flags.append('repetitive')
        
        return report
    
    def get_statistics(self, reports: List[QualityReport]) -> Dict[str, Any]:
        """Compute aggregate statistics from a batch of quality reports."""
        if not reports:
            return {'count': 0}
        
        admissible = [r for r in reports if r.is_admissible]
        
        # Per-dimension pass rates (no cross-domain aggregation)
        dim_pass_rates = {}
        for dim in self.thresholds:
            passing_dim = sum(1 for r in reports if r.dimension_gates.get(dim, False))
            dim_pass_rates[dim] = round(passing_dim / len(reports), 3)
        
        return {
            'count': len(reports),
            'admissible': len(admissible),
            'admissibility_rate': round(len(admissible) / len(reports), 3),
            'dimension_pass_rates': dim_pass_rates,
            'flag_counts': self._count_flags(reports),
        }
    
    def _count_flags(self, reports: List[QualityReport]) -> Dict[str, int]:
        """Count occurrences of each flag."""
        counts = {}
        for r in reports:
            for flag in r.flags:
                counts[flag] = counts.get(flag, 0) + 1
        return counts
